{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "$\\bullet$ Week 0 Wednesday Modelling using scikit-learn, Gensim, or other packages and APIs to model the topics       discussed in the tweets and their sentiments. You may use word clouds, k-mean clustering, etc. as a simple         model for topic modelling.\n",
    "    Unlike data wrangling, this is widely the interesting part in general data analysis. The step here are as         follows:\n",
    "\n",
    "$\\bullet$ determine the set of algorithms to try on the data (classification, regression, neural-net etc).\n",
    "\n",
    "$\\bullet$ model design - data splitting.\n",
    "\n",
    "$\\bullet$ model building\n",
    "\n",
    "$\\bullet$ evaluation (metrics)\n",
    "\n",
    "$\\bullet$ model review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "\n",
    "Importing all the required packages for this task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas library and other Python modules\n",
    "from nltk.corpus import stopwords, words # get stopwords from NLTK library & get all words in english language\n",
    "from nltk.tokenize import word_tokenize # to create word tokens\n",
    "from nltk.stem import WordNetLemmatizer # to reduce words to orginal form\n",
    "from nltk import pos_tag # For Parts of Speech tagging\n",
    "import string # Inbuilt string library\n",
    "#from emot.emo_unicode import UNICODE_EMOJI\n",
    "# WordCloud - Python linrary for creating image wordclouds\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "from PIL import Image # for opening, manipulating, and saving many different image file f\n",
    "from textblob import TextBlob \n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "from scipy.sparse import save_npz, load_npz # used for saving and loading sparse matrices\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-07 22:31:20+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @i_ameztoy: Extra random image (I):\\n\\nLets...</td>\n",
       "      <td>rt iameztoy extra random image lets focus one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.250000e-01</td>\n",
       "      <td>0.190625</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>City</td>\n",
       "      <td>i_ameztoy</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-08-07 22:31:16+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @IndoPac_Info: #China's media explains the ...</td>\n",
       "      <td>rt indopacinfo chinas media explains military ...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000e-01</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td></td>\n",
       "      <td>China, Taiwan</td>\n",
       "      <td>IndoPac_Info</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-08-07 22:31:07+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>China even cut off communication, they don't a...</td>\n",
       "      <td>china even cut communication dont anwer phonec...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>XiJinping</td>\n",
       "      <td>ZelenskyyUa</td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-08-07 22:31:06+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>Putin to #XiJinping : I told you my friend, Ta...</td>\n",
       "      <td>putin xijinping told friend taiwan vassal stat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>XiJinping</td>\n",
       "      <td></td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-08-07 22:31:04+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @ChinaUncensored: I’m sorry, I thought Taiw...</td>\n",
       "      <td>rt chinauncensored i’m sorry thought taiwan in...</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.938894e-18</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>381</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ChinaUncensored</td>\n",
       "      <td>Ayent, Schweiz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 created_at      source  \\\n",
       "0           0  2022-08-07 22:31:20+00:00  ['source']   \n",
       "1           1  2022-08-07 22:31:16+00:00  ['source']   \n",
       "2           2  2022-08-07 22:31:07+00:00  ['source']   \n",
       "3           3  2022-08-07 22:31:06+00:00  ['source']   \n",
       "4           4  2022-08-07 22:31:04+00:00  ['source']   \n",
       "\n",
       "                                       Original_Text  \\\n",
       "0  RT @i_ameztoy: Extra random image (I):\\n\\nLets...   \n",
       "1  RT @IndoPac_Info: #China's media explains the ...   \n",
       "2  China even cut off communication, they don't a...   \n",
       "3  Putin to #XiJinping : I told you my friend, Ta...   \n",
       "4  RT @ChinaUncensored: I’m sorry, I thought Taiw...   \n",
       "\n",
       "                                           full_text  sentiment      polarity  \\\n",
       "0  rt iameztoy extra random image lets focus one ...          0 -1.250000e-01   \n",
       "1  rt indopacinfo chinas media explains military ...          0 -1.000000e-01   \n",
       "2  china even cut communication dont anwer phonec...         -1  0.000000e+00   \n",
       "3  putin xijinping told friend taiwan vassal stat...          1  1.000000e-01   \n",
       "4  rt chinauncensored i’m sorry thought taiwan in...          0 -6.938894e-18   \n",
       "\n",
       "   subjectivity lang  favorite_count  retweet_count possibly_sensitive  \\\n",
       "0      0.190625   en               0              2                      \n",
       "1      0.100000   en               0            201                      \n",
       "2      0.000000   en               0              0                      \n",
       "3      0.350000   en               0              0                      \n",
       "4      0.556250   en               0            381                      \n",
       "\n",
       "        hashtags    user_mentions           place  \n",
       "0           City        i_ameztoy                  \n",
       "1  China, Taiwan     IndoPac_Info                  \n",
       "2      XiJinping      ZelenskyyUa     Netherlands  \n",
       "3      XiJinping                      Netherlands  \n",
       "4                 ChinaUncensored  Ayent, Schweiz  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tweets = pd.read_csv('../data/clean_processed_tweet.csv')\n",
    "model_tweets = model_tweets.fillna(\"\")\n",
    "model_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Sentiment Classifier using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/lazuxd/simple-imdb-sentiment-analysis/master/smiley.jpg\"/></center>\n",
    "<center><i>Image by AbsolutVision @ <a href=\"https://pixabay.com/ro/photos/smiley-emoticon-furie-sup%C4%83rat-2979107/\">pixabay.com</a></i></center>\n",
    "\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;**Sentiment analysis**, an important area in Natural Language Processing, is the process of automatically detecting affective states of text. Sentiment analysis is widely applied to voice-of-customer materials such as product reviews in online shopping websites like Amazon, movie reviews or social media. It can be just a basic task of classifying the polarity of a text as being positive/negative or it can go beyond polarity, looking at emotional states such as \"happy\", \"angry\", etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;After the dataset has been downloaded and extracted from archive we have to transform it into a more suitable form for feeding it into a machine learning model for training. We will start by combining all review data into 2 pandas Data Frames representing the train and test datasets, and then saving them as csv file.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The Data Frames will have the following form:  \n",
    "\n",
    "|text       |label      |\n",
    "|:---------:|:---------:|\n",
    "|review1    |0          |\n",
    "|review2    |1          |\n",
    "|review3    |1          |\n",
    "|.......    |...        |\n",
    "|reviewN    |0          |  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;where:  \n",
    "- review1, review2, ... = the actual text of movie review  \n",
    "- 0 = negative review  \n",
    "- 1 = positive review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>But machine learnng algorithms work only with numerical values.</b> We can't just input the text itself into a machine learning model and have it learn from that. We have to, somehow, <b>represent the text by numbers or vectors of numbers</b>. One way of doing this is by using the **Bag-of-words** model<sup>(3)</sup>, in which a piece of text (often called a **document**) is represented by a <b>vector of the counts of words from a vocabulary in that document. This model doesn't take into account grammar rules or word ordering; all it considers is the frequency of words</b>. If we use the counts of each word independently we name this representation a **unigram**. In general, in a **n-gram** we take into account the counts of <b>each combination of n words from the vocabulary that appears in a given document</b>.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For example, consider these two documents:  \n",
    "<br>  \n",
    "<div style=\"font-family: monospace;\"><center><b>d1: \"I am learning\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</b></center></div>  \n",
    "<div style=\"font-family: monospace;\"><center><b>d2: \"Machine learning is cool\"</b></center></div>  \n",
    "<br>\n",
    "The vocabulary of all words encountered in these two sentences is: \n",
    "\n",
    "<br/>  \n",
    "<div style=\"font-family: monospace;\"><center><b>v: [ I, am, learning, machine, is, cool ]</b></center></div>   \n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The unigram representations of d1 and d2:  \n",
    "<br>  \n",
    "\n",
    "|unigram(d1)|I       |am      |learning|machine |is      |cool    |\n",
    "|:---------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "|           |1       |1       |1       |0       |0       |0       |  \n",
    "\n",
    "|unigram(d2)|I       |am      |learning|machine |is      |cool    |\n",
    "|:---------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "|           |0       |0       |1       |1       |1       |1       |\n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;And, the bigrams of d1 and d2 are:\n",
    "  \n",
    "|bigram(d1) |I I     |I am    |I learning|...|machine am|machine learning|...|cool is|cool cool|\n",
    "|:---------:|:------:|:------:|:--------:|:-:|:--------:|:--------------:|:-:|:-----:|:-------:|\n",
    "|           |0       |1       |0         |...|0         |0               |...|0      |0        |  \n",
    "\n",
    "|bigram(d2) |I I     |I am    |I learning|...|machine am|machine learning|...|cool is|cool cool|\n",
    "|:---------:|:------:|:------:|:--------:|:-:|:--------:|:--------------:|:-:|:-----:|:-------:|\n",
    "|           |0       |0       |0         |...|0         |1               |...|0      |0        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Often, we can achieve slightly better results if instead of counts of words we use something called **term frequency times inverse document frequency** (or **tf-idf**). Maybe it sounds complicated, but it is not. Bear with me, I will explain this. The intuition behind this is the following. So, what's the problem of using just the frequency of terms inside a document? <b>Although some terms may have a high frequency inside documents they may not be so relevant for describing a given document in which they appear. That's because those terms may also have a high frequency across the collection of all documents</b>. For example, a collection of movie reviews may have terms specific to movies/cinematography that are present in almost all documents (they have a high **document frequency**). So, when we encounter those terms in a document this doesn't tell much about whether it is a positive or negative review. We need a way of relating **term frequency** (how frequent a term is inside a document) to **document frequency** (how frequent a term is across the whole collection of documents). That is:  \n",
    "  \n",
    "$$\\begin{align}\\frac{\\text{term frequency}}{\\text{document frequency}} &= \\text{term frequency} \\cdot \\frac{1}{\\text{document frequency}} \\\\ &= \\text{term frequency} \\cdot \\text{inverse document frequency} \\\\ &= \\text{tf} \\cdot \\text{idf}\\end{align}$$  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, there are more ways used to describe both term frequency and inverse document frequency. But the most common way is by putting them on a logarithmic scale:  \n",
    "  \n",
    "$$tf(t, d) = log(1+f_{t,d})$$  \n",
    "$$idf(t) = log(\\frac{1+N}{1+n_t})$$  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;where:  \n",
    "$$\\begin{align}f_{t,d} &= \\text{count of term } \\textbf{t} \\text{ in document } \\textbf{d} \\\\  \n",
    "N &= \\text{total number of documents} \\\\  \n",
    "n_t &= \\text{number of documents that contain term } \\textbf{t}\\end{align}$$  \n",
    "  \n",
    "<b>We added 1 in the first logarithm to avoid getting $-\\infty$ when $f_{t,d}$ is 0. In the second logarithm we added one fake document to avoid division by zero.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we transform our data into vectors of counts or tf-idf values we should remove English **stopwords**<sup>(6)(7)</sup>. <b>Stopwords are words that are very common in a language</b> and are usually removed in the preprocessing stage of natural text-related tasks like sentiment analysis or search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note that we should construct our vocabulary only based on the training set. When we will process the test data in order to make predictions we should use only the vocabulary constructed in the training phase, the rest of the words will be ignored.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, let's create the data frames and save them as csv files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, for the text vectorization part all the hard work is already done in the Scikit-Learn classes `CountVectorizer`<sup>(8)</sup> and `TfidfTransformer`<sup>(5)</sup>. We will use these classes to transform our csv files into unigram and bigram matrices (using both counts and tf-idf values). (<b>It turns out that if we only use a n-gram for a large n we don't get a good accuracy, we usually use all n-grams up to some n. So, when we say here bigrams we actually refer to uni+bigrams and when we say unigrams it's just unigrams.</b>) Each row in those matrices will represent a document (review) in our dataset, and each column will represent values associated with each word in the vocabulary (in the case of unigrams) or values associated with each combination of maximum 2 words in the vocabulary (bigrams).  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`CountVectorizer` has a parameter `ngram_range` which expects a tuple of size 2 that controls what n-grams to include. After we constructed a `CountVectorizer` object we should call `.fit()` method with the actual text as a parameter, in order for it to learn the required statistics of our collection of documents. Then, by calling `.transform()` method with our collection of documents it returns the matrix for the n-gram range specified. As the class name suggests, this matrix will contain just the counts. To obtain the tf-idf values, the class `TfidfTransformer` should be used. It has the `.fit()` and `.transform()` methods that are used in a similar way with those of `CountVectorizer`, but they take as input the counts matrix obtained in the previous step and `.transform()` will return a matrix with tf-idf values. We should use `.fit()` only on training data and then store these objects. When we want to evaluate the test score or whenever we want to make a prediction we should use these objects to transform the data before feeding it into our classifier.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Note that the matrices generated for our train or test data will be huge, and if we store them as normal numpy arrays they will not even fit into RAM. But most of the entries in these matrices will be zero. So, these Scikit-Learn classes are using Scipy sparse matrices<sup>(9)</sup> (`csr_matrix`<sup>(10)</sup> to be more exactly), which store just the non-zero entries and save a LOT of space.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We will use a linear classifier with stochastic gradient descent, `sklearn.linear_model.SGDClassifier`<sup>(11)</sup>, as our model. First we will generate and save our data in 4 forms: unigram and bigram matrix (with both counts and tf-idf values for each). Then we will train and evaluate our model for each these 4 data representations using `SGDClassifier` with the default parameters. After that, we choose the data representation which led to the best score and we will tune the hyper-parameters of our model with this data form using cross-validation in order to obtain the best results.\n",
    "\n",
    "<b>Refs:</b> \n",
    "* Convert a collection of text documents to a matrix of token counts: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "* Convert a collection of raw documents to a matrix of TF-IDF features: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_tweet_data = model_tweets.copy(deep=True)\n",
    "sentiment_analysis_tweet_data.drop(sentiment_analysis_tweet_data[sentiment_analysis_tweet_data['sentiment'] == -1].index, inplace=True)\n",
    "sentiment_analysis_tweet_data.reset_index(drop=True, inplace=True)\n",
    "tweet_train = sentiment_analysis_tweet_data.iloc[:4492,]\n",
    "tweet_test = sentiment_analysis_tweet_data.iloc[4493:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "unigram_vectorizer.fit(tweet_train['full_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rt': 1754,\n",
       " 'iameztoy': 1076,\n",
       " 'extra': 701,\n",
       " 'random': 1643,\n",
       " 'image': 1085,\n",
       " 'lets': 1235,\n",
       " 'focus': 761,\n",
       " 'one': 1462,\n",
       " 'specific': 1894,\n",
       " 'zone': 2321,\n",
       " 'western': 2247,\n",
       " 'coast': 398,\n",
       " 'gt': 866,\n",
       " 'longjing': 1263,\n",
       " 'district': 577,\n",
       " 'taichung': 1975,\n",
       " 'city': 380,\n",
       " 'ta': 1971,\n",
       " 'indopacinfo': 1102,\n",
       " 'chinas': 358,\n",
       " 'media': 1318,\n",
       " 'explains': 695,\n",
       " 'military': 1333,\n",
       " 'reasons': 1664,\n",
       " 'area': 152,\n",
       " 'drills': 595,\n",
       " 'taiwan': 1981,\n",
       " 'strait': 1932,\n",
       " 'read': 1654,\n",
       " 'labels': 1211,\n",
       " 'pi': 1522,\n",
       " 'putin': 1632,\n",
       " 'xijinping': 2298,\n",
       " 'told': 2061,\n",
       " 'friend': 797,\n",
       " 'vassal': 2170,\n",
       " 'state': 1914,\n",
       " 'including': 1095,\n",
       " 'nukes': 1438,\n",
       " 'much': 1376,\n",
       " 'like': 1244,\n",
       " 'ukrainian': 2128,\n",
       " 'model': 1354,\n",
       " 'warned': 2219,\n",
       " 'took': 2064,\n",
       " 'pelosi': 1510,\n",
       " 'open': 1466,\n",
       " 'eyes': 704,\n",
       " 'chinauncensored': 365,\n",
       " 'sorry': 1879,\n",
       " 'thought': 2035,\n",
       " 'independent': 1099,\n",
       " 'country': 465,\n",
       " 'government': 851,\n",
       " 'currency': 491,\n",
       " 'travel': 2082,\n",
       " 'benedictrogers': 222,\n",
       " 'must': 1385,\n",
       " 'let': 1234,\n",
       " 'happen': 875,\n",
       " 'ready': 1656,\n",
       " 'defend': 522,\n",
       " 'httpstcoz4rv925jhi': 1059,\n",
       " 'tgtmofficial': 2011,\n",
       " 'kind': 1194,\n",
       " 'connive': 432,\n",
       " 'extremely': 702,\n",
       " 'offensive': 1450,\n",
       " 'remarks': 1696,\n",
       " 'street': 1936,\n",
       " 'tsaiingwen': 2100,\n",
       " 'thegreattranslati': 2022,\n",
       " 'cgmeifangzhang': 335,\n",
       " 'chinese': 367,\n",
       " 'ambassador': 112,\n",
       " 'us': 2155,\n",
       " 'detailed': 548,\n",
       " 'usa': 2156,\n",
       " 'house': 930,\n",
       " 'speaker': 1888,\n",
       " 'nancy': 1387,\n",
       " 'pelosis': 1512,\n",
       " 'visit': 2193,\n",
       " 'opposed': 1473,\n",
       " 'china': 349,\n",
       " 'latest': 1221,\n",
       " 'pla': 1529,\n",
       " 'conducted': 426,\n",
       " 'massive': 1305,\n",
       " 'around': 158,\n",
       " 'response': 1715,\n",
       " 'serious': 1815,\n",
       " 'provocations': 1617,\n",
       " 'made': 1279,\n",
       " 'on': 1461,\n",
       " 'good': 848,\n",
       " 'infographic': 1104,\n",
       " 'missile': 1348,\n",
       " 'launches': 1224,\n",
       " 'august': 181,\n",
       " '4th': 31,\n",
       " 'chinataiwancrisis': 360,\n",
       " 'httpstcostzrr9fhu5': 1032,\n",
       " 'shenshiwei': 1827,\n",
       " 'roger': 1746,\n",
       " 'waters': 2233,\n",
       " 'theyre': 2028,\n",
       " 'encircling': 638,\n",
       " 'part': 1496,\n",
       " 'thats': 2017,\n",
       " 'absolutely': 53,\n",
       " 'accepted': 57,\n",
       " 'decentaz001': 513,\n",
       " 'marvininu': 1300,\n",
       " 'project': 1607,\n",
       " 'speaks': 1892,\n",
       " 'goodness': 849,\n",
       " 'future': 808,\n",
       " 'right': 1733,\n",
       " 'hodl': 912,\n",
       " 'win': 2260,\n",
       " 'marvin': 1299,\n",
       " 'coin': 400,\n",
       " 'eaglenews': 608,\n",
       " 'largestever': 1219,\n",
       " 'draw': 592,\n",
       " 'close': 391,\n",
       " 'diplomacy': 564,\n",
       " 'militarydrills': 1334,\n",
       " 'httpstcoyvl5n': 1057,\n",
       " 'timand2037': 2051,\n",
       " 'former': 777,\n",
       " 'colonial': 405,\n",
       " 'regimes': 1681,\n",
       " 'selfstyled': 1802,\n",
       " 'g7': 811,\n",
       " 'building': 287,\n",
       " 'false': 716,\n",
       " 'pretexts': 1592,\n",
       " 'war': 2215,\n",
       " 'terroralarm': 2010,\n",
       " 'surrounded': 1964,\n",
       " 'air': 87,\n",
       " 'sea': 1789,\n",
       " 'gears': 821,\n",
       " 'start': 1911,\n",
       " 'next': 1416,\n",
       " 'round': 1750,\n",
       " 'drill': 593,\n",
       " 'exer': 685,\n",
       " 'whole': 2255,\n",
       " 'international': 1118,\n",
       " 'community': 419,\n",
       " 'dont': 584,\n",
       " 'euobs': 669,\n",
       " 'opinion': 1470,\n",
       " 'honour': 922,\n",
       " 'commitment': 412,\n",
       " 'democracy': 535,\n",
       " 'especially': 664,\n",
       " 'ranked': 1644,\n",
       " 'freedom': 788,\n",
       " 'apple': 147,\n",
       " 'timcook': 2052,\n",
       " 'really': 1660,\n",
       " 'httpstcovogkgpq9r3': 1041,\n",
       " 'andrewjbates46': 126,\n",
       " 'carriedinterest': 312,\n",
       " 'loislerner': 1257,\n",
       " 'stock': 1925,\n",
       " 'tradesoptions': 2077,\n",
       " 'liars': 1237,\n",
       " 'parties': 1498,\n",
       " 'pathetic': 1503,\n",
       " 'although': 109,\n",
       " 'pro': 1600,\n",
       " 'marxism': 1301,\n",
       " 'definitely': 527,\n",
       " 'democrats': 537,\n",
       " 'httpstcob1yl7chlp0': 959,\n",
       " 'responding': 1714,\n",
       " 'cnn': 396,\n",
       " 'reporter': 1702,\n",
       " 'saying': 1779,\n",
       " 'busy': 292,\n",
       " 'interview': 1119,\n",
       " 'cofounder': 399,\n",
       " 'of': 1448,\n",
       " 'clear': 386,\n",
       " 'ar': 149,\n",
       " 'biden': 231,\n",
       " 'said': 1769,\n",
       " 'cant': 304,\n",
       " 'prevent': 1593,\n",
       " 'visiting': 2196,\n",
       " 'separation': 1811,\n",
       " 'powers': 1573,\n",
       " 'pale': 1485,\n",
       " 'excuse': 683,\n",
       " 'prevented': 1594,\n",
       " 'revmahoney': 1726,\n",
       " 'know': 1199,\n",
       " 'ccp': 325,\n",
       " 'would': 2285,\n",
       " 'treat': 2083,\n",
       " 'rest': 1717,\n",
       " 'world': 2279,\n",
       " 'easy': 616,\n",
       " 'answer': 136,\n",
       " 'look': 1265,\n",
       " 'treated': 2084,\n",
       " 'people': 1516,\n",
       " 'spokespersonchn': 1901,\n",
       " '66': 34,\n",
       " 'aircraft': 88,\n",
       " '14': 5,\n",
       " 'vessels': 2175,\n",
       " 'detected': 550,\n",
       " '7th': 39,\n",
       " 'taiwans': 1986,\n",
       " 'monitor': 1362,\n",
       " 'barkingmaad': 203,\n",
       " 'enemyinastate': 644,\n",
       " 'taiwanstraitscrisis': 1989,\n",
       " 'ovenready': 1481,\n",
       " 'distraction': 576,\n",
       " 'monkeypox': 1363,\n",
       " 'brexit': 275,\n",
       " 'deal': 506,\n",
       " 'borisjohnson': 264,\n",
       " 'williamyang120': 2258,\n",
       " 'long': 1261,\n",
       " 'bullied': 289,\n",
       " 'marginalized': 1292,\n",
       " 'isolated': 1141,\n",
       " 'banned': 202,\n",
       " 'organiz': 1477,\n",
       " 'veronic59122266': 2173,\n",
       " 'willing': 2259,\n",
       " 'risk': 1735,\n",
       " 'starting': 1913,\n",
       " 'make': 1284,\n",
       " 'profits': 1605,\n",
       " 'husbands': 1071,\n",
       " 'inside': 1105,\n",
       " 'you': 2309,\n",
       " 're': 1651,\n",
       " 'capitalism': 306,\n",
       " 'rich': 1729,\n",
       " 'amp': 122,\n",
       " 'communism': 416,\n",
       " 'muslims': 1384,\n",
       " 'educating': 620,\n",
       " 'extremists': 703,\n",
       " 'invasions': 1129,\n",
       " 'maybe': 1311,\n",
       " 'racist': 1640,\n",
       " 'supremacist': 1960,\n",
       " 'aka': 94,\n",
       " 'white': 2254,\n",
       " 'xinjiang': 2301,\n",
       " 'evil': 678,\n",
       " 'corfu': 454,\n",
       " 'greece': 859,\n",
       " 'summer': 1951,\n",
       " 'beaches': 214,\n",
       " 'excursion': 682,\n",
       " 'check': 344,\n",
       " 'availability': 188,\n",
       " 'httpstcobbzwjjwgjp': 960,\n",
       " 'vacanze': 2167,\n",
       " 'holidays': 917,\n",
       " 'ferie': 731,\n",
       " 'urlaub': 2154,\n",
       " 'today': 2058,\n",
       " 'studios': 1941,\n",
       " 'hotel': 928,\n",
       " '8august': 42,\n",
       " 'island': 1138,\n",
       " 'covid': 470,\n",
       " 'assange': 168,\n",
       " 'italy': 1147,\n",
       " 'trump': 2098,\n",
       " 'berlin': 224,\n",
       " 'london': 1260,\n",
       " 'milano': 1330,\n",
       " 'new': 1410,\n",
       " 'york': 2308,\n",
       " 'gaza': 819,\n",
       " 'httpstco3z2xd1codu': 947,\n",
       " 'substantially': 1945,\n",
       " 'increased': 1098,\n",
       " 'defense': 525,\n",
       " 'diplomatic': 565,\n",
       " 'contacts': 441,\n",
       " 'past': 1501,\n",
       " 'yrs': 2313,\n",
       " 'act': 60,\n",
       " 'clearly': 387,\n",
       " 'changing': 339,\n",
       " 'status': 1919,\n",
       " 'quo': 1638,\n",
       " 'across': 59,\n",
       " 'hypocrite': 1075,\n",
       " 'throw': 2041,\n",
       " 'mud': 1378,\n",
       " 'alarabiyaeng': 96,\n",
       " 'reiterates': 1686,\n",
       " 'won': 2275,\n",
       " 'succumb': 1948,\n",
       " 'pressure': 1588,\n",
       " 'days': 504,\n",
       " 'seas': 1791,\n",
       " 'surroundin': 1965,\n",
       " 'sunday': 1952,\n",
       " 'disputed': 575,\n",
       " 'reports': 1703,\n",
       " 'peoples': 1517,\n",
       " 'liberation': 1238,\n",
       " 'army': 157,\n",
       " 'warships': 2224,\n",
       " 'anonopsse': 134,\n",
       " 'think': 2032,\n",
       " 'stop': 1928,\n",
       " 'buying': 293,\n",
       " 'products': 1604,\n",
       " 'wont': 2276,\n",
       " 'putting': 1633,\n",
       " 'flag': 751,\n",
       " 'twitter': 2112,\n",
       " 'ha': 872,\n",
       " 'pauljawin': 1504,\n",
       " 'need': 1401,\n",
       " 'live': 1249,\n",
       " 'stream': 1935,\n",
       " 'slaughter': 1860,\n",
       " 'conflict': 429,\n",
       " 'kids': 1191,\n",
       " 'parents': 1492,\n",
       " 'see': 1796,\n",
       " 'guts': 871,\n",
       " 'splattered': 1899,\n",
       " 'real': 1658,\n",
       " 'time': 2053,\n",
       " 'tv': 2108,\n",
       " 'things': 2031,\n",
       " 'ano': 133,\n",
       " 'signs': 1846,\n",
       " 'ups': 2149,\n",
       " 'choose': 373,\n",
       " 'prison': 1598,\n",
       " 'deathc': 509,\n",
       " 'govt': 854,\n",
       " 'collapse': 403,\n",
       " 'stephenmcdonell': 1923,\n",
       " 'hardliners': 880,\n",
       " 'upper': 2148,\n",
       " 'ranks': 1645,\n",
       " 'communist': 417,\n",
       " 'party': 1499,\n",
       " 'beijing': 220,\n",
       " 'probably': 1601,\n",
       " 'happy': 876,\n",
       " 'vis': 2190,\n",
       " 'luck': 1275,\n",
       " 'httpstcoss4kz6guvf': 1031,\n",
       " 'kuomintang': 1208,\n",
       " 'plas': 1536,\n",
       " 'exercises': 687,\n",
       " 'immediate': 1087,\n",
       " 'threat': 2036,\n",
       " 'represented': 1705,\n",
       " 'may': 1309,\n",
       " 'ended': 640,\n",
       " 'precedent': 1579,\n",
       " 'set': 1818,\n",
       " 'the': 2018,\n",
       " 'eastern': 614,\n",
       " 'theater': 2019,\n",
       " 'command': 409,\n",
       " 'continues': 447,\n",
       " 'joint': 1167,\n",
       " 'focusing': 762,\n",
       " 'potus': 1568,\n",
       " 'genocidal': 825,\n",
       " '30': 21,\n",
       " 'trillion': 2091,\n",
       " 'debt': 511,\n",
       " 'sells': 1803,\n",
       " 'body': 257,\n",
       " 'door': 586,\n",
       " 'mat': 1306,\n",
       " 'korea': 1203,\n",
       " 'night': 1425,\n",
       " 'stand': 1908,\n",
       " 'flee': 755,\n",
       " 'day': 503,\n",
       " 'cancels': 301,\n",
       " 'oligarchs': 1460,\n",
       " 'invested': 1130,\n",
       " 'climatecrisis': 389,\n",
       " 'loose': 1266,\n",
       " '90': 43,\n",
       " 'fake': 711,\n",
       " 'investment': 1131,\n",
       " 'coming': 408,\n",
       " 'run': 1757,\n",
       " 'httpstcoh2digv8y31': 988,\n",
       " 'fallenxking': 714,\n",
       " 'true': 2097,\n",
       " 'wwiii': 2292,\n",
       " 'httpstco6z7b4cnakp': 951,\n",
       " 'thinkingpanda': 2033,\n",
       " 'pink': 1525,\n",
       " 'floyd': 756,\n",
       " 'got': 850,\n",
       " 'billion': 237,\n",
       " 'fans': 720,\n",
       " 'taiwanchina': 1982,\n",
       " 'pinkfloyd': 1526,\n",
       " 'rogerwaters': 1747,\n",
       " 'httpstcodvgozwwj2c': 971,\n",
       " '2156us': 20,\n",
       " '40': 27,\n",
       " 'offnew': 1454,\n",
       " 'ems': 637,\n",
       " 'intelligent': 1113,\n",
       " 'sleep': 1861,\n",
       " 'device': 552,\n",
       " 'fast': 722,\n",
       " 'hypnosis': 1073,\n",
       " 'insomnia': 1106,\n",
       " 'artifact': 163,\n",
       " 'wristband': 2286,\n",
       " 'watch': 2229,\n",
       " 'microcurrent': 1327,\n",
       " 'aid': 86,\n",
       " 'instrument': 1109,\n",
       " 'snoring': 1867,\n",
       " 'aliexpress': 100,\n",
       " 'httpstcotkdsd6gwf3': 1036,\n",
       " 'nancypelosi': 1388,\n",
       " 'uk': 2122,\n",
       " 'eua': 668,\n",
       " 'europe': 670,\n",
       " 'russia': 1759,\n",
       " 'america': 115,\n",
       " 'httpstcoqlqtasgyyz': 1022,\n",
       " 'australias': 185,\n",
       " 'shadow': 1822,\n",
       " 'defence': 521,\n",
       " 'minister': 1342,\n",
       " 'andrew': 125,\n",
       " 'hastie': 884,\n",
       " 'says': 1780,\n",
       " 'openminded': 1469,\n",
       " 'weighs': 2240,\n",
       " 'tr': 2075,\n",
       " 'youre': 2311,\n",
       " 'either': 627,\n",
       " 'moron': 1370,\n",
       " 'pay': 1505,\n",
       " 'vladimirputin': 2198,\n",
       " 'way': 2235,\n",
       " 'rnbreakfast': 1740,\n",
       " 'finally': 738,\n",
       " 'someone': 1875,\n",
       " 'balanced': 197,\n",
       " 'view': 2182,\n",
       " 'perceived': 1519,\n",
       " 'chinataiwan': 359,\n",
       " 'crisis': 481,\n",
       " 'reading': 1655,\n",
       " 'enoughroger': 652,\n",
       " 'rock': 1743,\n",
       " 'band': 199,\n",
       " 'httpstcof5pfhblizi': 979,\n",
       " 'fullyloaded': 803,\n",
       " 'f16': 705,\n",
       " 'vipers': 2187,\n",
       " 'armed': 155,\n",
       " 'deadly': 505,\n",
       " 'harpoon': 881,\n",
       " 'missiles': 1349,\n",
       " 'gearup': 822,\n",
       " 'hunt': 1070,\n",
       " 'intruding': 1124,\n",
       " 'taiwa': 1980,\n",
       " 'cdnposts': 327,\n",
       " 'games': 813,\n",
       " 'become': 218,\n",
       " 'backlogged': 194,\n",
       " 'global': 837,\n",
       " 'supply': 1954,\n",
       " 'chain': 337,\n",
       " 'max': 1308,\n",
       " 'keating': 1184,\n",
       " 'cdn': 326,\n",
       " 'militar': 1331,\n",
       " 'reuters': 1723,\n",
       " 'reported': 1701,\n",
       " 'commentator': 410,\n",
       " 'planning': 1535,\n",
       " 'hold': 913,\n",
       " 'regular': 1685,\n",
       " 'exe': 684,\n",
       " 'emilykschrader': 633,\n",
       " 'count': 459,\n",
       " 'consistently': 435,\n",
       " 'wrong': 2287,\n",
       " 'side': 1841,\n",
       " 'sides': 1842,\n",
       " 'worlds': 2281,\n",
       " 'largest': 1218,\n",
       " 'human': 1067,\n",
       " 'france': 784,\n",
       " 'say': 1778,\n",
       " 'reeducate': 1675,\n",
       " 'authorities': 187,\n",
       " 'education': 621,\n",
       " 'desinization': 542,\n",
       " 'population': 1556,\n",
       " 'effectively': 622,\n",
       " 'indoctrinated': 1101,\n",
       " 'intoxicated': 1123,\n",
       " 'reeducated': 1676,\n",
       " 'eliminate': 629,\n",
       " 'separatist': 1812,\n",
       " 'secessionist': 1792,\n",
       " 'theory': 2024,\n",
       " 'httpstco4ufkuy4cqi': 948,\n",
       " 'matijosaitismp': 1307,\n",
       " 'justification': 1178,\n",
       " 'use': 2160,\n",
       " 'speakerpelosi': 1889,\n",
       " 'pretext': 1591,\n",
       " 'aggressive': 82,\n",
       " 'acti': 61,\n",
       " 'chasewnelson': 343,\n",
       " 'bikhim': 236,\n",
       " 'hsiao': 934,\n",
       " 'escalation': 663,\n",
       " 'unreasonable': 2146,\n",
       " 'unnecessary': 2145,\n",
       " 'want': 2212,\n",
       " 'prc': 1578,\n",
       " 'dare': 499,\n",
       " 'invade': 1125,\n",
       " 'roc': 1742,\n",
       " 'bcos': 212,\n",
       " 'knows': 1200,\n",
       " 'full': 800,\n",
       " 'well': 2243,\n",
       " 'since': 1851,\n",
       " '19291949': 8,\n",
       " 'civil': 381,\n",
       " 'never': 1409,\n",
       " 'taken': 1992,\n",
       " 'able': 51,\n",
       " 'take': 1991,\n",
       " 'tw': 2109,\n",
       " 'invades': 1127,\n",
       " 'trigger': 2089,\n",
       " 'demise': 534,\n",
       " '3rd': 26,\n",
       " 'term': 2007,\n",
       " 'xi': 2295,\n",
       " 'httpstcovvdark09yn': 1043,\n",
       " 'communists': 418,\n",
       " 'that': 2016,\n",
       " 'dictator': 557,\n",
       " 'jinping': 1160,\n",
       " 'rape': 1646,\n",
       " 'torture': 2068,\n",
       " 'murder': 1379,\n",
       " 'genocide': 826,\n",
       " 'learned': 1230,\n",
       " 'mao': 1289,\n",
       " 'mass': 1303,\n",
       " 'murderers': 1380,\n",
       " 'keeptaiwanccpfree': 1186,\n",
       " 'globaltimesnews': 840,\n",
       " 'httpstcogsonaptxgd': 987,\n",
       " 'signed': 1844,\n",
       " 'potsdamproclamation': 1567,\n",
       " '1945': 9,\n",
       " 'always': 110,\n",
       " 'consistent': 434,\n",
       " 'policy': 1548,\n",
       " 'line': 1246,\n",
       " 'proclamation': 1603,\n",
       " 'taking': 1993,\n",
       " 'back': 191,\n",
       " 'penghu': 1515,\n",
       " 'islands': 1139,\n",
       " 'defeated': 520,\n",
       " 'japan': 1154,\n",
       " 'httpstcoovihhrj7ln': 1015,\n",
       " 'elonmusk': 631,\n",
       " 'esaagar': 661,\n",
       " 'joerogan': 1164,\n",
       " 'jack': 1149,\n",
       " 'shocks': 1833,\n",
       " 'none': 1428,\n",
       " 'talk': 1996,\n",
       " 'peace': 1508,\n",
       " 'get': 830,\n",
       " 'bogeyman': 258,\n",
       " 'construct': 438,\n",
       " 'russiachinairan': 1760,\n",
       " 'etcgtampalso': 666,\n",
       " 'americans': 119,\n",
       " 'destroy': 545,\n",
       " 'worldwhen': 2283,\n",
       " 'toastamp90': 2057,\n",
       " 'globe': 841,\n",
       " 'isnt': 1140,\n",
       " 'involved': 1133,\n",
       " 'influencers': 1103,\n",
       " 'understand': 2138,\n",
       " 'worldgtitll': 2280,\n",
       " 'usaukeuampa': 2159,\n",
       " 'cites': 377,\n",
       " 'selfdefined': 1799,\n",
       " 'enemies': 642,\n",
       " 'countriesgtitll': 464,\n",
       " 'unannouncedmultinationalmultipronged': 2133,\n",
       " 'attackusing': 177,\n",
       " 'deployedamppredeployed': 540,\n",
       " 'biocyberampnuke': 238,\n",
       " 'pretend': 1589,\n",
       " 'surprised': 1963,\n",
       " 'tried': 2087,\n",
       " 'every': 675,\n",
       " 'means': 1314,\n",
       " 'warn': 2218,\n",
       " 'th': 2012,\n",
       " 'debidsan': 510,\n",
       " 'weapons': 2237,\n",
       " 'httpstcosnjs6hj1ou': 1030,\n",
       " 'via': 2178,\n",
       " 'youtube': 2312,\n",
       " 'ronanltynan': 1749,\n",
       " 'troubling': 2096,\n",
       " 'coverage': 467,\n",
       " 'failure': 710,\n",
       " 'issue': 1144,\n",
       " 'nhkworldnews': 1420,\n",
       " 'lodged': 1255,\n",
       " 'protest': 1611,\n",
       " 'united': 2140,\n",
       " 'states': 1918,\n",
       " 'light': 1243,\n",
       " 'militarys': 1336,\n",
       " 'ma': 1277,\n",
       " 'frenchjeanguy1': 795,\n",
       " 'defconwsalerts': 518,\n",
       " 'hope': 923,\n",
       " 'nations': 1393,\n",
       " 'regional': 1683,\n",
       " 'neighbours': 1403,\n",
       " 'plusand': 1542,\n",
       " 'aukus': 182,\n",
       " 'prepar': 1583,\n",
       " 'kikipedialol': 1192,\n",
       " 'welcome': 2241,\n",
       " 'province': 1615,\n",
       " 'httpstcoci5hpvcdit': 968,\n",
       " '301military': 22,\n",
       " 'fired': 744,\n",
       " 'rocket': 1744,\n",
       " 'firing': 745,\n",
       " 'httpstcoydfg77bsfv': 1056,\n",
       " 'seriously': 1816,\n",
       " 'burn': 291,\n",
       " 'damn': 497,\n",
       " 'holy': 918,\n",
       " 'books': 263,\n",
       " 'theyve': 2029,\n",
       " 'warping': 2221,\n",
       " 'minds': 1339,\n",
       " 'centuries': 331,\n",
       " 'events': 673,\n",
       " 'described': 541,\n",
       " 'thus': 2043,\n",
       " 'lack': 1212,\n",
       " 'proper': 1609,\n",
       " 'context': 443,\n",
       " 'pontifex': 1553,\n",
       " 'opening': 1468,\n",
       " 'vault': 2171,\n",
       " 'existentialreckoning': 688,\n",
       " 'ufoarmy': 2118,\n",
       " 'theres': 2026,\n",
       " 'legitimacy': 1232,\n",
       " 'claim': 383,\n",
       " 'formed': 776,\n",
       " '1949': 11,\n",
       " 'governs': 853,\n",
       " 'cn': 394,\n",
       " '1912': 7,\n",
       " 'usurped': 2164,\n",
       " 'mainland': 1281,\n",
       " 'form': 775,\n",
       " 'forms': 778,\n",
       " 'definitions': 528,\n",
       " 'remains': 1695,\n",
       " 'rocs': 1745,\n",
       " 'httpstcoerl6i3ki0b': 975,\n",
       " 'spectacle': 1895,\n",
       " 'options': 1474,\n",
       " 'narrow': 1389,\n",
       " 'winning': 2263,\n",
       " 'xijinpingthepooh': 2300,\n",
       " 'pelosivisit': 1513,\n",
       " 'pelosivisittotaiwan': 1514,\n",
       " 'hongkong': 920,\n",
       " 'httpstcox7e43hsusw': 1048,\n",
       " 'ktrtrs': 1206,\n",
       " 'uncle': 2136,\n",
       " 'responsible': 1716,\n",
       " 'chinesevirus': 370,\n",
       " 'caused': 323,\n",
       " 'delay': 529,\n",
       " 'zelensky': 2316,\n",
       " 'threatens': 2037,\n",
       " 'ukrainians': 2129,\n",
       " 'accuses': 58,\n",
       " 'traitor': 2078,\n",
       " 'poor': 1554,\n",
       " 'nomoreweapon': 1427,\n",
       " 'stopwar': 1929,\n",
       " 'ukraine': 2125,\n",
       " 'freeukraine': 793,\n",
       " 'otancriminal': 1479,\n",
       " 'nato': 1394,\n",
       " 'bfmtv': 228,\n",
       " 'cnews': 395,\n",
       " 'lci': 1227,\n",
       " 'afp': 76,\n",
       " 'morandini': 1366,\n",
       " 'sievierodonetsk': 1843,\n",
       " 'odessa': 1447,\n",
       " 'olevnika': 1458,\n",
       " 'china2asean': 350,\n",
       " '15': 6,\n",
       " 'aug': 180,\n",
       " 'scampfm': 1781,\n",
       " 'wangyi': 2211,\n",
       " 'held': 895,\n",
       " 'press': 1587,\n",
       " 'conference': 427,\n",
       " 'foreign': 772,\n",
       " 'attending': 178,\n",
       " 'series': 1814,\n",
       " 'mini': 1340,\n",
       " '4449us': 29,\n",
       " '91': 45,\n",
       " 'off2022': 1449,\n",
       " 'nfc': 1417,\n",
       " 'smart': 1864,\n",
       " 'men': 1320,\n",
       " '390390': 24,\n",
       " 'screen': 1787,\n",
       " 'display': 574,\n",
       " 'bluetooth': 252,\n",
       " 'call': 296,\n",
       " 'local': 1254,\n",
       " 'music': 1381,\n",
       " 'smartwatch': 1865,\n",
       " 'huawei': 1065,\n",
       " 'xiaomibox': 2297,\n",
       " 'watches': 2231,\n",
       " 'httpstco03chixjk6a': 939,\n",
       " 'unitedstates': 2141,\n",
       " 'httpstcoir1usu9oip': 997,\n",
       " 'morning': 1369,\n",
       " 'east': 613,\n",
       " 'rift': 1732,\n",
       " 'vally': 2169,\n",
       " 'unaffected': 2131,\n",
       " 'sky': 1857,\n",
       " 'aviation': 189,\n",
       " 'activities': 64,\n",
       " 'recently': 1667,\n",
       " 'hotballoons': 927,\n",
       " 'taitung': 1979,\n",
       " 'httpstco0azcje5epp': 940,\n",
       " 'sanction': 1770,\n",
       " 'daily': 496,\n",
       " 'httpstconumuicwooh': 1008,\n",
       " 'strife': 1938,\n",
       " 'put': 1631,\n",
       " 'energy': 645,\n",
       " 'eggs': 623,\n",
       " 'russian': 1762,\n",
       " 'basket': 206,\n",
       " 'auspol': 184,\n",
       " 'summary': 1950,\n",
       " 'week': 2238,\n",
       " 'minutes': 1345,\n",
       " 'plus': 1541,\n",
       " 'satire': 1775,\n",
       " 'alexjones': 98,\n",
       " 'hillaryclinton': 904,\n",
       " 'rishisunak': 1734,\n",
       " 'conservativeleadershipcontest': 433,\n",
       " 'surprise': 1962,\n",
       " 'end': 639,\n",
       " 'enjoy': 649,\n",
       " 'httpstcoxwezpdmclj': 1053,\n",
       " 'allies': 103,\n",
       " 'rattled': 1649,\n",
       " 'dangerous': 498,\n",
       " 'moment': 1359,\n",
       " 'working': 2278,\n",
       " 'constitution': 437,\n",
       " 'ignoring': 1082,\n",
       " 'bipartisan': 239,\n",
       " 'warmongers': 2217,\n",
       " 'watching': 2232,\n",
       " 'senators': 1806,\n",
       " 'push': 1630,\n",
       " 'support': 1955,\n",
       " 'httpstcomrltalixyr': 1005,\n",
       " 'politico': 1551,\n",
       " 'pmls2scgmuenoig': 1544,\n",
       " 'merry': 1323,\n",
       " 'heart': 891,\n",
       " 'medicine': 1319,\n",
       " 'snort': 1868,\n",
       " 'chortle': 374,\n",
       " 'tad': 1972,\n",
       " 'laugh': 1222,\n",
       " 'httpstcoohzs6eqj': 1013,\n",
       " 'militarylandnet': 1335,\n",
       " 'raises': 1642,\n",
       " 'question': 1635,\n",
       " 'impending': 1089,\n",
       " 'invasion': 1128,\n",
       " 'progress': 1606,\n",
       " 'technology': 2002,\n",
       " 'taiwanese': 1984,\n",
       " 'film': 736,\n",
       " 'broadcast': 281,\n",
       " 'beach': 213,\n",
       " 'landing': 1214,\n",
       " 'directly': 566,\n",
       " 'zhoulichn': 2318,\n",
       " 'türkiye': 2116,\n",
       " 'firmly': 746,\n",
       " 'committed': 414,\n",
       " 'onechinapolicy': 1463,\n",
       " 'also': 108,\n",
       " 'xinjiangrelated': 2302,\n",
       " 'americanmcgee': 118,\n",
       " 'maya276309': 1310,\n",
       " 'caitoz': 294,\n",
       " 'sadly': 1765,\n",
       " 'living': 1252,\n",
       " 'bubble': 286,\n",
       " 'youll': 2310,\n",
       " 'find': 740,\n",
       " 'unleashed': 2143,\n",
       " 'severe': 1819,\n",
       " 'crackdown': 473,\n",
       " 'religion': 1692,\n",
       " 'cultural': 487,\n",
       " 'revolution': 1727,\n",
       " 'heres': 898,\n",
       " 'report': 1700,\n",
       " 'cswuk': 486,\n",
       " 'many': 1288,\n",
       " 'others': 1480,\n",
       " 'httpstcoswa1rjggki': 1033,\n",
       " 'dramvrostramao': 591,\n",
       " 'quick': 1637,\n",
       " 'interesting': 1116,\n",
       " 'fact': 706,\n",
       " 'historically': 906,\n",
       " '400': 28,\n",
       " 'years': 2305,\n",
       " 'ago': 83,\n",
       " 'populated': 1555,\n",
       " 'abo': 52,\n",
       " 'mofataiwan': 1357,\n",
       " 'wu': 2290,\n",
       " 'extended': 700,\n",
       " 'warm': 2216,\n",
       " 'great': 858,\n",
       " 'supporter': 1957,\n",
       " 'comraderalph': 422,\n",
       " 'privilege': 1599,\n",
       " 'pm': 1543,\n",
       " 'breaking': 270,\n",
       " 'news': 1413,\n",
       " 'oxford': 1484,\n",
       " 'dictionary': 559,\n",
       " 'urged': 2152,\n",
       " 'add': 69,\n",
       " 'touch': 2070,\n",
       " 'edition': 619,\n",
       " 'ability': 50,\n",
       " 'turn': 2107,\n",
       " 'anything': 142,\n",
       " 'shit': 1832,\n",
       " 'pretending': 1590,\n",
       " 'save': 1776,\n",
       " 'portfolio': 1558,\n",
       " 'cause': 322,\n",
       " 'provocation': 1616,\n",
       " 'tries': 2088,\n",
       " 'blame': 244,\n",
       " 'misdeeds': 1346,\n",
       " 'httpstcosb1db3zg4u': 1027,\n",
       " 'libijian2': 1239,\n",
       " 'land': 1213,\n",
       " 'attack': 173,\n",
       " 'lon': 1259,\n",
       " 'juanyongjy': 1174,\n",
       " 'donal': 582,\n",
       " 'launched': 1223,\n",
       " 'scathing': 1782,\n",
       " 'attacks': 176,\n",
       " 'crewtvnet': 477,\n",
       " 'arrives': 160,\n",
       " 'ratchets': 1647,\n",
       " 'activity': 65,\n",
       " 'ft': 798,\n",
       " 'httpstcoerjqojmqy7': 974,\n",
       " 'forces': 771,\n",
       " 'conduct': 425,\n",
       " 'livefire': 1250,\n",
       " 'artillery': 166,\n",
       " 'excercises': 681,\n",
       " 'south': 1880,\n",
       " '11': 3,\n",
       " 'india': 1100,\n",
       " 'leverage': 1236,\n",
       " 'order': 1475,\n",
       " 'globalrecession': 839,\n",
       " 'globalisation': 838,\n",
       " 'httpstcozostyp0yxs': 1063,\n",
       " 'content': 442,\n",
       " 'analysis': 124,\n",
       " 'article': 162,\n",
       " 'scores': 1785,\n",
       " '86100': 41,\n",
       " 'complete': 420,\n",
       " 'viewed': 2183,\n",
       " 'httpstcowu2l9rusqc': 1047,\n",
       " 'im': 1084,\n",
       " 'bot': 265,\n",
       " 'httpstcovt2zu355f6': 1042,\n",
       " 'thanks': 2015,\n",
       " 'clue': 393,\n",
       " 'humanity': 1068,\n",
       " 'course': 466,\n",
       " 'mean': 1313,\n",
       " 'everything': 677,\n",
       " 'everyone': 676,\n",
       " 'don': 581,\n",
       " 'culture': 489,\n",
       " 'social': 1870,\n",
       " 'breakthrough': 272,\n",
       " 'only': 1464,\n",
       " 'clean': 385,\n",
       " 'mess': 1324,\n",
       " 'caption': 307,\n",
       " 'fixstatusquonow': 750,\n",
       " 'greenlifecycle': 860,\n",
       " 'httpstcod5xpusykjn': 970,\n",
       " 'josephwongut': 1170,\n",
       " 'kmt': 1198,\n",
       " 'elders': 628,\n",
       " 'leaders': 1229,\n",
       " 'welcomed': 2242,\n",
       " 'concerning': 423,\n",
       " 'economic': 618,\n",
       " 'charm': 342,\n",
       " 'mariocavolo': 1295,\n",
       " 'lie': 1240,\n",
       " 'defensive': 526,\n",
       " 'enhanced': 648,\n",
       " 'protect': 1610,\n",
       " 'sovereignty': 1884,\n",
       " 'their': 2023,\n",
       " 'neloangelo314': 1404,\n",
       " 'planned': 1534,\n",
       " 'go': 842,\n",
       " 'plan': 1531,\n",
       " '48': 30,\n",
       " 'hours': 929,\n",
       " 'exercise': 686,\n",
       " 'httpstcozehgm1grar': 1062,\n",
       " 'chinaemberitrea': 354,\n",
       " 'ministry': 1343,\n",
       " 'affairs': 75,\n",
       " 'announces': 131,\n",
       " 'countermeasures': 460,\n",
       " ...}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(unigram_vectorizer.vocabulary_.items(), columns=['Vocabulary', 'Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2324 entries, 0 to 2323\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Vocabulary  2324 non-null   object\n",
      " 1   Frequency   2324 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 36.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"Frequency\", axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>شیطانکےچیلے</td>\n",
       "      <td>2323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>zoom</td>\n",
       "      <td>2322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>zone</td>\n",
       "      <td>2321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>zlj517</td>\n",
       "      <td>2320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>zjz</td>\n",
       "      <td>2319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>zhoulichn</td>\n",
       "      <td>2318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>zhangheqing</td>\n",
       "      <td>2317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>zelensky</td>\n",
       "      <td>2316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>yummy</td>\n",
       "      <td>2315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>yudhvirjaswal</td>\n",
       "      <td>2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>yrs</td>\n",
       "      <td>2313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>youtube</td>\n",
       "      <td>2312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>youre</td>\n",
       "      <td>2311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>youll</td>\n",
       "      <td>2310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>you</td>\n",
       "      <td>2309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>york</td>\n",
       "      <td>2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>ymedia</td>\n",
       "      <td>2307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>yi</td>\n",
       "      <td>2306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>years</td>\n",
       "      <td>2305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>yearn</td>\n",
       "      <td>2304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Vocabulary  Frequency\n",
       "1543    شیطانکےچیلے       2323\n",
       "1656           zoom       2322\n",
       "9              zone       2321\n",
       "1033         zlj517       2320\n",
       "2183            zjz       2319\n",
       "848       zhoulichn       2318\n",
       "1998    zhangheqing       2317\n",
       "724        zelensky       2316\n",
       "1437          yummy       2315\n",
       "1491  yudhvirjaswal       2314\n",
       "296             yrs       2313\n",
       "641         youtube       2312\n",
       "458           youre       2311\n",
       "861           youll       2310\n",
       "245             you       2309\n",
       "287            york       2308\n",
       "1489         ymedia       2307\n",
       "1711             yi       2306\n",
       "881           years       2305\n",
       "1284          yearn       2304"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>20220722</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>2022</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>2020</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>200</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>1985</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>1974</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>1967</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>1949</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>1948</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1945</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>19291949</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>1912</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>1030am</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Vocabulary  Frequency\n",
       "1973   20220722         19\n",
       "1913       2022         18\n",
       "2089       2020         17\n",
       "1935       2019         16\n",
       "1993        200         15\n",
       "1927       1985         14\n",
       "1900       1974         13\n",
       "1899       1967         12\n",
       "697        1949         11\n",
       "1086       1948         10\n",
       "585        1945          9\n",
       "554    19291949          8\n",
       "700        1912          7\n",
       "745          15          6\n",
       "215          14          5\n",
       "1266         12          4\n",
       "936          11          3\n",
       "1777     1030am          2\n",
       "1808     100000          1\n",
       "1650        100          0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tf_idf_transformer = TfidfTransformer()\n",
    "unigram_tf_idf_transformer.fit(X_train_unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> fit_transform </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_train_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "bigram_vectorizer.fit(tweet_train['full_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram = bigram_vectorizer.transform(tweet_train['full_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "bigram_tf_idf_transformer.fit(X_train_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y,train_size=0.75, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "\n",
    "    global_vars = globals()\n",
    "    if(valid_score > global_vars['best_score']):\n",
    "        global_vars['best_model'] = clf\n",
    "        global_vars['best_model_name'] = title\n",
    "        global_vars['best_score'] = valid_score\n",
    "\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = tweet_train['sentiment'].values\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Counts\n",
      "Train score: 1.0 ; Validation score: 0.92\n",
      "\n",
      "Unigram Tf-Idf\n",
      "Train score: 1.0 ; Validation score: 0.89\n",
      "\n",
      "Bigram Counts\n",
      "Train score: 1.0 ; Validation score: 0.84\n",
      "\n",
      "Bigram Tf-Idf\n",
      "Train score: 1.0 ; Validation score: 0.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = \"\"\n",
    "best_model_name = \"\"\n",
    "best_score = 0\n",
    "\n",
    "train_and_show_scores(X_train_unigram, y_train, 'Unigram Counts')\n",
    "train_and_show_scores(X_train_unigram_tf_idf, y_train, 'Unigram Tf-Idf')\n",
    "train_and_show_scores(X_train_bigram, y_train, 'Bigram Counts')\n",
    "train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;The best data form seems to be **ungram with Validation score** as it gets the highest validation accuracy: **0.92**; we will use it next for hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Model is Unigram Counts with a Validation score of: 0.92\n"
     ]
    }
   ],
   "source": [
    "print(f'The best Model is {best_model_name} with a Validation score of: {round(best_score, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_using_model(best_model: SGDClassifier, model_type: str):\n",
    "    unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "    unigram_vectorizer.fit(tweet_test['full_text'].values)\n",
    "    X_test_unigram = unigram_vectorizer.transform(tweet_test['full_text'].values)\n",
    "\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    bigram_vectorizer.fit(tweet_test['full_text'].values)\n",
    "    X_test_bigram = bigram_vectorizer.transform(tweet_test['full_text'].values)\n",
    "\n",
    "    y_test = tweet_test['sentiment'].values\n",
    "\n",
    "    if(model_type == \"Unigram Counts\"):\n",
    "        X_test = X_test_unigram\n",
    "\n",
    "    elif(model_type == \"Unigram Tf-Idf\"):\n",
    "        unigram_tf_idf_transformer = TfidfTransformer()\n",
    "        unigram_tf_idf_transformer.fit(X_test_unigram)\n",
    "        X_test_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_test_unigram)\n",
    "\n",
    "        X_test = X_test_unigram_tf_idf\n",
    "\n",
    "    elif(model_type == \"Bigram Counts\"):\n",
    "        X_test = X_test_bigram\n",
    "\n",
    "    else:\n",
    "        bigram_tf_idf_transformer = TfidfTransformer()\n",
    "        bigram_tf_idf_transformer.fit(X_test_bigram)\n",
    "\n",
    "        X_test_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_test_bigram)\n",
    "        X_test = X_test_bigram_tf_idf\n",
    "\n",
    "   \n",
    "    return best_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving generated LDA\n",
    "sgd = joblib.dump(best_model, '../data/newsentimentSGDmodel.jl')\n",
    "# lda_model = joblib.load('lda_model.jl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, for each data form we split it into train & validation sets, train a `SGDClassifier` and output the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 684 kB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gensim in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (4.2.0)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scipy in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: jinja2 in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (2.11.3)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: joblib in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (1.20.3)\n",
      "Requirement already satisfied: setuptools in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: future in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: numexpr in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: scikit-learn in /home/success/anaconda3/lib/python3.9/site-packages (from pyLDAvis) (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/success/anaconda3/lib/python3.9/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/success/anaconda3/lib/python3.9/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/success/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/success/anaconda3/lib/python3.9/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/success/anaconda3/lib/python3.9/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/success/anaconda3/lib/python3.9/site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn\n",
      "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136882 sha256=673143ccc792fe501319bb8669d1249b9f2d6828df65660d63c47f7bda17bfcb\n",
      "  Stored in directory: /home/success/.cache/pip/wheels/57/a4/86/d10c6c2e0bf149fbc0afb0aa5a6528ac35b30a133a0270c477\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=8fe1bc60b0ca92b7a98048b3824d5b9afa23d3710ceb12a8451a12e5be9c8dd7\n",
      "  Stored in directory: /home/success/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built pyLDAvis sklearn\n",
      "Installing collected packages: sklearn, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install spacy\n",
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>full_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-08-07 22:31:20+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @i_ameztoy: Extra random image (I):\\n\\nLets...</td>\n",
       "      <td>rt iameztoy extra random image lets focus one ...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.250000e-01</td>\n",
       "      <td>0.190625</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>City</td>\n",
       "      <td>i_ameztoy</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-08-07 22:31:16+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @IndoPac_Info: #China's media explains the ...</td>\n",
       "      <td>rt indopacinfo chinas media explains military ...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000e-01</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td></td>\n",
       "      <td>China, Taiwan</td>\n",
       "      <td>IndoPac_Info</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-08-07 22:31:07+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>China even cut off communication, they don't a...</td>\n",
       "      <td>china even cut communication dont anwer phonec...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>XiJinping</td>\n",
       "      <td>ZelenskyyUa</td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-08-07 22:31:06+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>Putin to #XiJinping : I told you my friend, Ta...</td>\n",
       "      <td>putin xijinping told friend taiwan vassal stat...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>XiJinping</td>\n",
       "      <td></td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-08-07 22:31:04+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @ChinaUncensored: I’m sorry, I thought Taiw...</td>\n",
       "      <td>rt chinauncensored i’m sorry thought taiwan in...</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.938894e-18</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>381</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ChinaUncensored</td>\n",
       "      <td>Ayent, Schweiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>2022-08-07 20:34:08+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @RobertQ84643496: POLYMATECH INVESTS US$ 1 ...</td>\n",
       "      <td>rt robertq84643496 polymatech invests us 1 bil...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>semiconductor</td>\n",
       "      <td>RobertQ84643496</td>\n",
       "      <td>Virginia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>2022-08-07 20:34:05+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @SpokespersonCHN: \"#Taiwan is part of China...</td>\n",
       "      <td>rt spokespersonchn taiwan part china thats abs...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.333333e-01</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "      <td></td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>SpokespersonCHN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>2022-08-07 20:34:02+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @IndoPac_Info: From Chinese media:\\n\\n\"#PLA...</td>\n",
       "      <td>rt indopacinfo chinese media pla eastern theat...</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.000000e-02</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td></td>\n",
       "      <td>PLA, Taiwan</td>\n",
       "      <td>IndoPac_Info</td>\n",
       "      <td>奈良県 奈良市 Nara, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>2022-08-07 20:33:52+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @IndoPac_Info: 1) A Lithuanian delegation h...</td>\n",
       "      <td>rt indopacinfo 1 lithuanian delegation headed ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td></td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>IndoPac_Info</td>\n",
       "      <td>奈良県 奈良市 Nara, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>2022-08-07 20:33:40+00:00</td>\n",
       "      <td>['source']</td>\n",
       "      <td>RT @EpochOpinion: Opinion💭by Roger L. Simon\\n\\...</td>\n",
       "      <td>rt epochopinion opinionby roger l simon ccp ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.062500e-01</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td></td>\n",
       "      <td>CCP</td>\n",
       "      <td>EpochOpinion, SpeakerPelosi</td>\n",
       "      <td>Somewhere, U.S.A.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                 created_at      source  \\\n",
       "0             0  2022-08-07 22:31:20+00:00  ['source']   \n",
       "1             1  2022-08-07 22:31:16+00:00  ['source']   \n",
       "2             2  2022-08-07 22:31:07+00:00  ['source']   \n",
       "3             3  2022-08-07 22:31:06+00:00  ['source']   \n",
       "4             4  2022-08-07 22:31:04+00:00  ['source']   \n",
       "..          ...                        ...         ...   \n",
       "994         994  2022-08-07 20:34:08+00:00  ['source']   \n",
       "995         995  2022-08-07 20:34:05+00:00  ['source']   \n",
       "996         996  2022-08-07 20:34:02+00:00  ['source']   \n",
       "997         997  2022-08-07 20:33:52+00:00  ['source']   \n",
       "998         998  2022-08-07 20:33:40+00:00  ['source']   \n",
       "\n",
       "                                         Original_Text  \\\n",
       "0    RT @i_ameztoy: Extra random image (I):\\n\\nLets...   \n",
       "1    RT @IndoPac_Info: #China's media explains the ...   \n",
       "2    China even cut off communication, they don't a...   \n",
       "3    Putin to #XiJinping : I told you my friend, Ta...   \n",
       "4    RT @ChinaUncensored: I’m sorry, I thought Taiw...   \n",
       "..                                                 ...   \n",
       "994  RT @RobertQ84643496: POLYMATECH INVESTS US$ 1 ...   \n",
       "995  RT @SpokespersonCHN: \"#Taiwan is part of China...   \n",
       "996  RT @IndoPac_Info: From Chinese media:\\n\\n\"#PLA...   \n",
       "997  RT @IndoPac_Info: 1) A Lithuanian delegation h...   \n",
       "998  RT @EpochOpinion: Opinion💭by Roger L. Simon\\n\\...   \n",
       "\n",
       "                                             full_text  sentiment  \\\n",
       "0    rt iameztoy extra random image lets focus one ...          0   \n",
       "1    rt indopacinfo chinas media explains military ...          0   \n",
       "2    china even cut communication dont anwer phonec...         -1   \n",
       "3    putin xijinping told friend taiwan vassal stat...          1   \n",
       "4    rt chinauncensored i’m sorry thought taiwan in...          0   \n",
       "..                                                 ...        ...   \n",
       "994  rt robertq84643496 polymatech invests us 1 bil...         -1   \n",
       "995  rt spokespersonchn taiwan part china thats abs...          1   \n",
       "996  rt indopacinfo chinese media pla eastern theat...          0   \n",
       "997  rt indopacinfo 1 lithuanian delegation headed ...         -1   \n",
       "998  rt epochopinion opinionby roger l simon ccp ma...          0   \n",
       "\n",
       "         polarity  subjectivity lang  favorite_count  retweet_count  \\\n",
       "0   -1.250000e-01      0.190625   en               0              2   \n",
       "1   -1.000000e-01      0.100000   en               0            201   \n",
       "2    0.000000e+00      0.000000   en               0              0   \n",
       "3    1.000000e-01      0.350000   en               0              0   \n",
       "4   -6.938894e-18      0.556250   en               0            381   \n",
       "..            ...           ...  ...             ...            ...   \n",
       "994  0.000000e+00      0.000000   en               0              2   \n",
       "995  1.333333e-01      0.433333   en               0            669   \n",
       "996 -5.000000e-02      0.050000   en               0             60   \n",
       "997  0.000000e+00      0.000000   en               0             66   \n",
       "998 -4.062500e-01      0.750000   en               0             21   \n",
       "\n",
       "    possibly_sensitive       hashtags                user_mentions  \\\n",
       "0                                City                    i_ameztoy   \n",
       "1                       China, Taiwan                 IndoPac_Info   \n",
       "2                           XiJinping                  ZelenskyyUa   \n",
       "3                           XiJinping                                \n",
       "4                                                  ChinaUncensored   \n",
       "..                 ...            ...                          ...   \n",
       "994              False  semiconductor              RobertQ84643496   \n",
       "995                            Taiwan              SpokespersonCHN   \n",
       "996                       PLA, Taiwan                 IndoPac_Info   \n",
       "997                            Taiwan                 IndoPac_Info   \n",
       "998                               CCP  EpochOpinion, SpeakerPelosi   \n",
       "\n",
       "                   place  \n",
       "0                         \n",
       "1                         \n",
       "2            Netherlands  \n",
       "3            Netherlands  \n",
       "4         Ayent, Schweiz  \n",
       "..                   ...  \n",
       "994             Virginia  \n",
       "995                       \n",
       "996  奈良県 奈良市 Nara, JAPAN  \n",
       "997  奈良県 奈良市 Nara, JAPAN  \n",
       "998    Somewhere, U.S.A.  \n",
       "\n",
       "[999 rows x 15 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_data = model_tweets.copy(deep=True)\n",
    "topic_model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hastags_words_list():\n",
    "    hashtagList = []\n",
    "    for hashtags in topic_model_data.hashtags:\n",
    "        if(hashtags != \"\"):\n",
    "            hashtagList += hashtags.split(',')\n",
    "\n",
    "    return hashtagList\n",
    "\n",
    "hashtag = get_hastags_words_list()\n",
    "\n",
    "data = [word for sentence in topic_model_data.full_text for word in sentence.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['City', 'China', ' Taiwan', 'XiJinping', 'XiJinping']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt', 'iameztoy', 'extra', 'random', 'image']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words = data + hashtag\n",
    "data_words = [word for word in data_words if word != '']\n",
    "data_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 't']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readable View\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keyword of topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "perplexity_score = lda_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', perplexity_score)  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lda_model, '../data/newtopicLDAmodel.jl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> TUTORIAL </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using the processed twitter data from yesterday's challenge</h2>.\n",
    "\n",
    "\n",
    "- Form a new data frame (named `cleanTweet`), containing columns $\\textbf{clean-text}$ and $\\textbf{polarity}$.\n",
    "\n",
    "- Write a function `text_category` that takes a value `p` and returns, depending on the value of p, a string `'positive'`, `'negative'` or `'neutral'`.\n",
    "\n",
    "- Apply this function (`text_category`) on the $\\textbf{polarity}$ column of `cleanTweet` in 1 above to form a new column called $\\textbf{score}$ in `cleanTweet`.\n",
    "\n",
    "- Visualize The $\\textbf{score}$ column using piechart and barchart\n",
    "\n",
    "<h5>Now we want to build a classification model on the clean tweet following the steps below:</h5>\n",
    "\n",
    "* Remove rows from `cleanTweet` where $\\textbf{polarity}$ $= 0$ (i.e where $\\textbf{score}$ = Neutral) and reset the frame index.\n",
    "* Construct a column $\\textbf{scoremap}$ Use the mapping {'positive':1, 'negative':0} on the $\\textbf{score}$ column\n",
    "* Create feature and target variables `(X,y)` from $\\textbf{clean-text}$ and $\\textbf{scoremap}$ columns respectively.\n",
    "* Use `train_test_split` function to construct `(X_train, y_train)` and `(X_test, y_test)` from `(X,y)`\n",
    "\n",
    "* Build an `SGDClassifier` model from the vectorize train text data. Use `CountVectorizer()` with a $\\textit{trigram}$ parameter.\n",
    "\n",
    "* Evaluate your model on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cross-Validation for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;For this part we will use `RandomizedSearchCV`<sup>(12)</sup> which chooses the parameters randomly from the list that we give, or according to the distribution that we specify from `scipy.stats` (e.g. uniform); then is estimates the test error by doing cross-validation and after all iterations we can find the best estimator, the best parameters and the best score in the variables `best_estimator_`, `best_params_` and `best_score_`.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Because the search space for the parameters that we want to test is very big and it may need a huge number of iterations until it finds the best combination, we will split the set of parameters in 2 and do the hyper-parameter tuning process in two phases. First we will find the optimal combination of loss, learning_rate and eta0 (i.e. initial learning rate); and then for penalty and alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "* Flask\n",
    "* Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<sup>(1)</sup> &nbsp;[Sentiment Analysis - Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis)  \n",
    "<sup>(2)</sup> &nbsp;[Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)  \n",
    "<sup>(3)</sup> &nbsp;[Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)  \n",
    "<sup>(4)</sup> &nbsp;[Tf-idf - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)  \n",
    "<sup>(5)</sup> &nbsp;[TfidfTransformer - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)  \n",
    "<sup>(6)</sup> &nbsp;[Stop words - Wikipedia](https://en.wikipedia.org/wiki/Stop_words)  \n",
    "<sup>(7)</sup> &nbsp;[A list of English stopwords](https://gist.github.com/sebleier/554280)  \n",
    "<sup>(8)</sup> &nbsp;[CountVectorizer - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)  \n",
    "<sup>(9)</sup> &nbsp;[Scipy sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html)  \n",
    "<sup>(10)</sup> [Compressed Sparse Row matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)  \n",
    "<sup>(11)</sup> [SGDClassifier - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)  \n",
    "<sup>(12)</sup> [RandomizedSearchCV - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)  \n",
    "<sup>(13)</sup> [Sentiment Classification using Document Embeddings trained with\n",
    "Cosine Similarity](https://www.aclweb.org/anthology/P19-2057.pdf)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
